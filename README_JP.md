# Intelligent Knowledge Distiller (インテリジェント知識蒸留器)

このプロジェクトは、研究論文 **「From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases」** ([arXiv:2506.17484](https://arxiv.org/abs/2506.17484)) で提案された、マルチエージェントによる知識ベース構築システムの実装です。

サポートチケットのような非構造化テキストデータを、大規模言語モデル（LLM）を用いて、構造化され、要約された知識ベースへと変換します。このアプリケーションは、プロセスを制御するためのユーザーインターフェースを提供し、GeminiとOpenAIの両モデルに対応しています。

## 特徴

- **自動ナレッジベース構築**: 生のテキストを、カテゴリ分類され、要約されたナレッジ記事へと変換します。
- **動的カテゴリ分類**: データからカテゴリとサブカテゴリを自動的に発見します。
- **階層的処理**: アイテム数が多いカテゴリに対しては、サブカテゴリを作成して処理することで、より深い分析を行い、詳細が失われるのを防ぎます。
- **LLMの柔軟性**: GoogleのGeminiとOpenAIのモデルの両方をサポートしています。
- **カスタマイズ可能な処理**:
    - **ドメイン指定**: ユーザーはビジネスドメイン（例：「サプライチェーン」）を指定して、AIの理解を調整できます。
    - **処理モード**:
        - **シンプルモード**: 事前定義されたプロンプトを使用して処理します。
        - **ダイナミックモード**: 指定されたドメインに基づいてプロンプトを最適化し、より高品質な結果を目指します。
- **多言語対応**: ユーザーインターフェースは日本語と英語の両方で利用可能です。
- **柔軟なエンドポイント設定**: OpenAIおよびAzure OpenAI (AOAI) などの互換APIに対応するため、APIキー、ベースURL、モデル名（デプロイメント名）をUIから柔軟に設定できます。

## 設定

右上の歯車アイコンをクリックすると、設定モーダルが開きます。

### AIモデルプロバイダー

使用するLLMプロバイダーを選択します。

- **GEMINI**: GoogleのGeminiモデルを使用します。APIキーは `.env` ファイルで設定する必要があります。
    - **モデルの変更**:
        現在、使用されるGeminiモデルはソースコード内で直接指定されています。モデルを変更するには、`services/geminiService.ts` ファイル内の `generateContent` メソッドにある `model` の値を、希望するモデル名（例: `'gemini-2.5-flash-preview-04-17'`）に書き換えてください。
- **OPENAI**: OpenAIのモデル、または互換API（Azure OpenAIなど）を使用します。

### OpenAI / Azure OpenAI (AOAI) の設定

プロバイダーとして `OPENAI` を選択すると、以下の項目が設定可能になります。

- **OpenAI API Key**:
  - APIキーを入力します。
  - ここで入力したキーは、環境変数（`.env.local` ファイル）に設定されたキーよりも優先されます。
  - 空欄の場合は、環境変数に設定された `OPENAI_API_KEY` が使用されます。

- **OpenAI API Base URL**:
  - APIのエンドポイントURLを指定します。
  - **Azure OpenAI (AOAI) を使用する場合**: 完全なエンドポイントURLをそのまま貼り付けることができます。アプリケーションが自動的に処理します。
    - 例: `https://example-aoai.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_ID/chat/completions?api-version=2024-02-01`

- **OpenAI Model (任意)**:
  - モデル名を指定します。
  - **Azure OpenAI (AOAI) を使用する場合**: ここに **デプロイメント名** を入力します。（例: `gpt-4.1-nano`）
  - 空欄の場合、またはURLからモデル名を自動抽出できない場合は、デフォルトのモデルが使用されます。

### プロキシとCORSの処理

このアプリケーションは、ブラウザから外部API（Azure OpenAIなど）へ直接リクエストを送信する際に発生するCORS（Cross-Origin Resource Sharing）エラーを回避するため、バックエンドにAPIリクエストを中継するプロキシサーバーを内蔵しています。

- **ローカル開発環境 (`npm run dev`)**:
  Vite開発サーバーと同時に、`server.js`で実装されたNode.jsプロキシサーバーが起動します。フロントエンドからの `/api` で始まるリクエストは、このプロキシサーバーに転送されます。

- **本番環境 (Docker / Cloud Run)**:
  コンテナ内では、静的ファイルを配信する **Nginx** と、APIリクエストを中継する **Node.jsプロキシサーバー** の両方が起動します。Nginxが受け取ったリクエストのうち、`/api` で始まるものはNode.jsプロキシサーバーに転送されます。

このアーキテクチャにより、環境を問わずCORSの問題を透過的に解決しています。

## アーキテクチャと処理フロー

このシステムは、RAG（Retrieval-Augmented Generation）を強化するための「オフラインファースト」アプローチを採用しています。問い合わせ時に複雑な処理を行うのではなく、事前に高品質で凝縮された知識ベースを構築します。これは、特化したAIエージェントのパイプラインによって実現されます。

### エージェントの役割

1.  **カテゴリ発見エージェント**: 生のデータを分析し、知識の「カテゴリ」分類体系を特定・作成します。
2.  **チケット分類エージェント**: 各チケットを最も関連性の高いカテゴリに割り当てます。
3.  **知識合成エージェント**: カテゴリ（またはサブカテゴリ）内の全チケットを集約し、簡潔で実用的な知識記事を生成します。

### 大規模カテゴリに対する階層的処理

この実装の重要な特徴は、大量のチケットを含むカテゴリを処理する能力です。カテゴリ内のチケット数がしきい値 (`SUBCATEGORY_THRESHOLD`) を超えると、システムは動的により詳細な階層的処理フローに切り替わります。

1.  **サブカテゴリ発見**: 特化したエージェントが大規模カテゴリ内のチケットを再分析し、より粒度の細かいサブカテゴリを発見します。
2.  **サブカテゴリ分類**: チケットは、これらの新しいサブカテゴリに分類されます。
3.  **サブカテゴリごとの合成**: サブカテゴリごとに知識記事が生成され、より焦点の合った具体的な洞察が得られます。

この動的なロジックは、`App.tsx` 内の `handleProcess` 関数で制御されています。

## 技術スタック

- **フロントエンド**: React, TypeScript, Vite
- **スタイリング**: Tailwind CSS
- **LLM連携**: `@google/genai`, `openai`
- **国際化**: `i18next`, `react-i18next`

## 主要なコードの場所

| 機能 | 場所 | 説明 |
| :--- | :--- | :--- |
| **コアプロンプト** | `constants.ts` | AIエージェントが使用するすべてのプロンプトテンプレートが含まれています。 |
| **オーケストレーションロジック** | `App.tsx` | メインの `handleProcess` 関数が、パイプライン全体の状態とフローを制御します。 |
| **LLMサービス** | `services/` | `llmService.ts` がインターフェースを定義し、`geminiService.ts` と `openaiService.ts` が具体的な実装を提供します。 |
| **型定義** | `types.ts` | `Category` や `KnowledgeArticle` などのコアデータ構造を定義します。 |
| **UIコンポーネント** | `components/` | ユーザーインターフェースを構成するReactコンポーネントが含まれています。 |
| **多言語対応** | `public/locales/` | 日本語と英語の翻訳用JSONファイルが含まれています。 |

## ローカルでの実行方法

**前提条件:** Node.js

1.  リポジトリをクローンします。
2.  依存関係をインストールします:
    ```bash
    npm install
    ```
3.  ルートディレクトリに `.env.local` ファイルを作成します（`.env.example` があればコピーし、なければ新規作成します）。そして、必要に応じてAPIキーを設定します。
    ```
    # Geminiを使用する場合に必要
    GEMINI_API_KEY="YOUR_GEMINI_API_KEY"

    # OpenAIを使用する場合、UIで設定しない場合はここに設定
    OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
    ```
    **注**: OpenAIのAPIキーは、UIの設定画面から直接入力することも可能です。UIで入力したキーは、この `.env.local` ファイルの設定よりも優先されます。
4.  開発サーバーを実行します:
    ```bash
    npm run dev
    ```
5.  ブラウザを開き、表示されたローカルURL（通常は `http://localhost:5173`）にアクセスします。
